# graphpipe-tf - Serve TensorFlow Models via Graphpipe

The headlines are true! You can serve your TF models via graphpipe
easily using this server.

## Quickstart

Prereqs:
  - libtensorflow: https://www.tensorflow.org/install/install_go or `make install-tensorflow`
  - govendor: https://github.com/kardianos/govendor or `make install-govendor`


```
  export RUN_TYPE=cpu
  make deps && make

  # or

  make in-docker  # just do it all in docker
```

Should get you a CPU-build, or tell you what dependencies you need
to do so (see Prereqs above).

For the GPU build the same more or less applies but you need to have
all the Nvidia CUDA stuff installed and supported on your system (or
just have it supported, and have nvidia-docker).  We have tested against
CUDA 8 and CUDA 9.

```
  export RUN_TYPE=gpu
  make deps && make

  # or

  make in-docker  # just do it all in docker
```

## Running the server
Before running the server, you first need a tensorflow model to serve.
The model should be in protobuf (.pb) format, or in the tensorflow-serving
directory format.  We provide a tool to easily convert keras to the correct
model format [here](https://github.com/oracle/graphpipe-tf-py/blob/master/examples/convert.py).

To see the server's command line options:

```
./graphpipe-tf --help
graphpipe-tf - serving up ml models

Usage:
  graphpipe-tf [flags]

Flags:
  -n, --cache            do not cache results
  -d, --dir string       dir for local state (default "~/.graphpipe-tf")
  -h, --help             help for graphpipe-tf
  -i, --inputs string    comma seprated default inputs
  -l, --listen string    listen string (default "127.0.0.1:9000")
  -m, --model string     tensorflow model to load (accepts local files and unauthenticated http/https urls)
  -o, --outputs string   comma separated default outputs
  -v, --verbose          verbose output
  -V, --version          show version
```

The only required parameter is --model (see above).  If not specified, inputs and outputs
will be automatically determined by the graphdef as the first input and last output; if your
model has multiple inputs and/or outputs, you must specify these parameters manually.

Once you have your model, start the server .

```
./graphpipe-tf --model=mymodel.pb
```

## Environment Variables
For convenience, the key parameters of the service can be configured with environment variables,
 GP_MODEL, GP_INPUTS, GP_OUTPUTS, and GP_CACHE.

## Building Containers

If you're a developer of graphpipe, you may need to at some point
build a new dev-container:

```
  make dev-container
```

And if you are deploying changes, you may need to make a new runtime
container:

```
  make runtime-container
```

And if things seem broken, try dropping into a shell in your dev
container and figure things out:

```
  make shell
```

## Troubleshooting

### govendor can't fetch private libs
The in-docker setup _should_ forward `ssh-agent`s correctly if you
have it set up on your systems. Don't forget to `ssh-add` your key!

This link might be helpful: https://developer.github.com/v3/guides/using-ssh-agent-forwarding/

### proxies :(
Proxying should be forwarded for all our commands, but you may need
to configure your docker runtime to use them as well. Probably lives
(or needs to be created) at:

  `/etc/systemd/system/docker.service.d/http-proxy.conf`
